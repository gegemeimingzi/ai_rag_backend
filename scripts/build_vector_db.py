import os
import re
import warnings
from tqdm import tqdm
from langchain.schema import Document
from langchain_chroma import Chroma
from models.embed_model import load_embedding_model



def extract_source_text(text: str) -> str:
    """
    æå–æ³•æ¡ç¼–å·ï¼Œä¾‹å¦‚ã€Šä¸­åäººæ°‘å…±å’Œå›½å®ªæ³•ã€‹ç¬¬äº”åä¹æ¡ => ç¬¬äº”åä¹æ¡
    """
    match = re.search(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¡", text)
    return match.group(0) if match else ""

def load_txt_documents_with_metadata(directory: str):
    """
    åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶ï¼Œå¹¶å°†å…¶æŒ‰è¡Œåˆ‡åˆ†ä¸ºå¤šä¸ª Documentï¼Œé™„å¸¦æ³•æ¡ç¼–å·å’Œæ–‡ä»¶å…ƒä¿¡æ¯ã€‚
    """
    documents = []
    file_paths = []

    for root, _, files in os.walk(directory):
        for filename in files:
            if filename.endswith(".txt"):
                file_paths.append(os.path.join(root, filename))

    for full_path in tqdm(file_paths, desc="ğŸ“‚ åŠ è½½æ³•å¾‹æ–‡æœ¬æ–‡ä»¶"):
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                full_text = f.read()

            file_name = os.path.splitext(os.path.basename(full_path))[0]
            entries = [para.strip() for para in full_text.strip().split("\n") if para.strip()]

            for entry in entries:
                source_text = extract_source_text(entry)
                metadata = {
                    "source_file": full_path,
                    "source_name": file_name,
                    "source_text": source_text
                }
                documents.append(Document(page_content=entry, metadata=metadata))

        except UnicodeDecodeError as e:
            print(f"âš ï¸ æ–‡ä»¶è¯»å–å¤±è´¥: {full_path}ï¼Œé”™è¯¯: {e}")

    return documents


if __name__ == "__main__":
    # âœ… åŠ è½½åµŒå…¥æ¨¡å‹
    embedding = load_embedding_model()

    # âœ… å‘é‡å­˜å‚¨è·¯å¾„é…ç½®
    persist_dir = r"C:\Users\30300\Desktop\Rag_Project\chroma_persist"
    collection_name = "legal_docs"

    if os.path.exists(persist_dir) and os.listdir(persist_dir):
        print("âœ… æ£€æµ‹åˆ°å·²æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨åŠ è½½...")
        vectordb = Chroma(
            embedding_function=embedding,
            persist_directory=persist_dir,
            collection_name=collection_name
        )
    else:
        print("ğŸš€ æœªæ£€æµ‹åˆ°å‘é‡åº“ï¼Œå¼€å§‹åŠ è½½æ–‡æœ¬å¹¶æ„å»ºæ–°åº“...")
        data_dir = r"C:\Users\30300\Desktop\Rag_Project\data\Chinese-Laws"
        raw_documents = load_txt_documents_with_metadata(data_dir)
        print(f"âœ… æˆåŠŸåŠ è½½ {len(raw_documents)} æ¡æ³•å¾‹æ¡æ–‡")

        vectordb = Chroma.from_documents(
            documents=tqdm(raw_documents, desc="ğŸ“Œ æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å†™å…¥æ•°æ®åº“"),
            embedding=embedding,
            persist_directory=persist_dir,
            collection_name=collection_name
        )
        print(f"âœ… å‘é‡æ•°æ®åº“å·²æ„å»ºå®Œæ¯•ï¼Œå…±åŒ…å« {len(raw_documents)} æ¡æ³•æ¡ç‰‡æ®µ")

    # âœ… ç¤ºä¾‹æ£€ç´¢
    print("\nğŸ“Œ ç¤ºä¾‹æŸ¥è¯¢ç»“æœï¼š")
    results = vectordb.similarity_search("å·¥ä¼šæƒç›Š", k=3)
    for i, doc in enumerate(results, start=1):
        print(f"\næ–‡æ¡£ {i}:")
        print("å†…å®¹æ‘˜è¦:", doc.page_content[:100].replace("\n", " "))
        print("æ¥æºæ–‡ä»¶:", doc.metadata.get("source_name"))
        print("æ³•æ¡æ¥æº:", doc.metadata.get("source_text"))
